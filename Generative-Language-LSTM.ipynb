{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Generative Language model using LSTM RNN's\n",
    "* Idea here is to build a character level language model that will predict next character\n",
    "* Adapted from code in Jeremy Howards' fastai\n",
    "* Leverages Pytorch nn.LSTM\n",
    "* We will leverage text from Nietzsche for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.column_data import *\n",
    "from torchtext import vocab, data\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/nietzsche/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "text=open(f'{PATH}nietzsche.txt').read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nietzsche.txt  \u001b[0m\u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "TRN_PATH='trn/'\n",
    "VAL_PATH='val/'\n",
    "TRN=f'{PATH}{TRN_PATH}'\n",
    "VAL=f'{PATH}{VAL_PATH}'\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(f'{PATH}nietzsche.txt').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split file into 20% at end being validation and rest is training\n",
    "# above is more indicative of reality\n",
    "with open(f'{PATH}nietzsche.txt') as f1:\n",
    "    for i,line in enumerate(f1.readlines()):\n",
    "        if i<7948:\n",
    "            with open(f'{TRN}trn.txt','a') as f2:\n",
    "                f2.write(line)\n",
    "        else:\n",
    "            with open(f'{VAL}val.txt','a') as f3:\n",
    "                f3.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987\n"
     ]
    }
   ],
   "source": [
    "with open(f'{VAL}val.txt') as foo:\n",
    "    lines = len(foo.readlines())\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size identifies the number of chunks text is broken up into\n",
    "### BPTT identifies the number of layers to backprop through\n",
    "### n_fac identifies the size of the embedding matrix\n",
    "### n_hidden identifies the number of hidden layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=data.Field(lower=True, tokenize=list)\n",
    "bs=64\n",
    "bptt=8\n",
    "n_fac=42\n",
    "n_hidden=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES=dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md=LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 55, 1, 482972)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nl is number of hidden layers\n",
    "### we introduce dropout in LSTM to improve performance\n",
    "### initialize both hidden and cell state to zero in init_hidden\n",
    "### last batch likely smaller than 'bs' when we can reset init_hidden for next epoch\n",
    "### repackage essentially keeps the activation history but not the order of operations or else we will have a terribly long back prop to do and can lead to gradient explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size, self.nl=vocab_size, nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.lstm=nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out=nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self,cs):\n",
    "        bs=cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs:\n",
    "            self.init_hidden(bs)\n",
    "        outp, h = self.lstm(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl,bs,n_hidden)),V(torch.zeros(self.nl,bs,n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=CharSeqStatefulLSTM(md.nt, n_fac, 512,2).cuda()\n",
    "lo=LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4441408628f74c95ac97296b708afa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.815847   1.731781  \n",
      "    1      1.716238   1.649038                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.64904])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging callbacks (which is why we need Layer Optimizer) to enable SGDR without creating a learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1d2c483b1848a3aa626edc3d24a9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.541419   1.491363  \n",
      "    1      1.589831   1.520836                               \n",
      "    2      1.465451   1.431938                               \n",
      "    3      1.614292   1.542708                               \n",
      "    4      1.531411   1.483209                               \n",
      "    5      1.444086   1.418692                               \n",
      "    6      1.380124   1.383623                               \n",
      "    7      1.565699   1.52324                                \n",
      "    8      1.551937   1.51495                                \n",
      "    9      1.519852   1.481465                               \n",
      "    10     1.473957   1.451115                               \n",
      "    11     1.443204   1.423085                               \n",
      "    12     1.386805   1.387693                               \n",
      "    13     1.336657   1.360653                               \n",
      "    14     1.308315   1.346684                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.34668])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end=lambda sched, cycle: save_model(m, f'language/models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648d2ca3186a4891b29215a15f5402c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.256145   1.330343  \n",
      "    1      1.26011    1.330412                               \n",
      "    2      1.261839   1.330382                               \n",
      "    3      1.261137   1.330381                               \n",
      "    4      1.253866   1.330374                               \n",
      "    5      1.258794   1.33043                                \n",
      "    6      1.261633   1.330494                               \n",
      "    7      1.259962   1.330445                               \n",
      "    8      1.258886   1.330375                               \n",
      "    9      1.26144    1.330397                               \n",
      "    10     1.253788   1.330318                               \n",
      "    11     1.258093   1.33046                                \n",
      "    12     1.260009   1.33047                                \n",
      "    13     1.262138   1.330401                               \n",
      "    14     1.260007   1.330477                               \n",
      "    15     1.25585    1.330248                               \n",
      "    16     1.258868   1.330374                               \n",
      "    17     1.255181   1.330252                               \n",
      "    18     1.262735   1.330262                               \n",
      "    19     1.252162   1.330235                               \n",
      "    20     1.257681   1.330285                               \n",
      "    21     1.253507   1.330252                               \n",
      "    22     1.257276   1.330741                               \n",
      "    23     1.255198   1.330165                               \n",
      "    24     1.259022   1.330181                               \n",
      "    25     1.263339   1.330273                               \n",
      "    26     1.257325   1.330222                               \n",
      "    27     1.254753   1.330198                               \n",
      "    28     1.261781   1.330213                               \n",
      "    29     1.259305   1.330267                               \n",
      "    30     1.25699    1.330085                               \n",
      "    31     1.261506   1.330314                               \n",
      "    32     1.263392   1.330216                               \n",
      "    33     1.263162   1.330147                               \n",
      "    34     1.258572   1.330185                               \n",
      "    35     1.260899   1.330195                               \n",
      "    36     1.259209   1.330232                               \n",
      "    37     1.253514   1.330189                               \n",
      "    38     1.26033    1.330086                               \n",
      "    39     1.250625   1.330173                               \n",
      "    40     1.255984   1.33014                                \n",
      "    41     1.258863   1.330136                               \n",
      "    42     1.259079   1.33015                                \n",
      "    43     1.258409   1.330004                               \n",
      "    44     1.260837   1.330069                               \n",
      "    45     1.255853   1.331735                               \n",
      "    46     1.259328   1.329993                               \n",
      "    47     1.255435   1.330014                               \n",
      "    48     1.252402   1.329988                               \n",
      "    49     1.254388   1.32998                                \n",
      "    50     1.256051   1.330045                               \n",
      "    51     1.257114   1.330026                               \n",
      "    52     1.259985   1.32998                                \n",
      "    53     1.255705   1.330099                               \n",
      "    54     1.25469    1.3301                                 \n",
      "    55     1.259886   1.329981                               \n",
      "    56     1.254456   1.329994                               \n",
      "    57     1.257744   1.3301                                 \n",
      "    58     1.261358   1.330027                               \n",
      "    59     1.259582   1.329949                               \n",
      "    60     1.257271   1.329997                               \n",
      "    61     1.255182   1.329919                               \n",
      "    62     1.253868   1.330021                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.33002])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end=lambda sched, cycle: save_model(m, f'language/models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs=TEXT.numericalize(inp)\n",
    "    p=m(VV(idxs.transpose(0,1)))\n",
    "    r=torch.multinomial(p[-1].exp(),1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp,n):\n",
    "    res=inp\n",
    "    for i in range(n):\n",
    "        c=get_next(inp)\n",
    "        res += c\n",
    "        inp=inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those freedom) is not ye. we cannot aveagatishingermitia seriousness, perhaps loud it,. our appearhips itself made a recognizance of nature. Ã¤=a rome peraits sympathy and 'every am.129. the art parelyly and greatteets alas!he makes the invidogation), and fash,through and experient--he knows that we have tolose, when the wide monstants and betray in the intellecture, indeed, twick stands in which we ha\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those ciece withthe sharp in means of the merrestinctions about theproblem our ownunplaced the excitation, which is, deceive cause and something, itis prevail alsofootenon for a reason in ravers on the future,mediocre, their heart (and whlew unreally, theconserve of them, in aristower--is! in we zost about woman, such ancience (it _we acquired the progred lattering, and in their philosophers: for thei\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=CharSeqStatefulLSTM(md.nt, n_fac, 512,2).cuda()\n",
    "lo=LayerOptimizer(optim.Adam, m, 1e-3, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14d96c0f45542ab850a842eff95e9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.703501   1.632261  \n",
      "    1      1.530248   1.492523                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.49252])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24de2e5af16430d9ad8206732463b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.412499   1.414315  \n",
      "    1      1.413593   1.405016                               \n",
      "    2      1.34115    1.372035                               \n",
      "    3      1.393105   1.397299                               \n",
      "    4      1.326587   1.371936                               \n",
      "    5      1.273308   1.350991                               \n",
      "    6      1.240319   1.347031                               \n",
      "    7      1.325724   1.376638                               \n",
      "    8      1.284922   1.372995                               \n",
      "    9      1.245337   1.364623                               \n",
      "    10     1.213546   1.36058                                \n",
      "    11     1.182966   1.35611                                \n",
      "    12     1.147278   1.354585                               \n",
      "    13     1.128629   1.353648                               \n",
      "    14     1.11418    1.354798                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.3548])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end=lambda sched, cycle: save_model(m, f'language/models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17568f355de4f3f848f148e49d8edbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.110234   1.355215  \n",
      "    1      1.110402   1.355278                               \n",
      "    2      1.107677   1.3553                                 \n",
      "    3      1.108815   1.355562                               \n",
      "    4      1.104336   1.355876                               \n",
      "    5      1.104404   1.35604                                \n",
      "    6      1.114028   1.356071                               \n",
      "    7      1.102969   1.356021                               \n",
      "    8      1.100136   1.355995                               \n",
      "    9      1.098343   1.356731                               \n",
      "    10     1.096388   1.357007                               \n",
      "    11     1.100279   1.357219                               \n",
      "    12     1.096348   1.357292                               \n",
      "    13     1.097496   1.357357                               \n",
      "    14     1.1014     1.357462                               \n",
      "    15     1.104543   1.356908                               \n",
      "    16     1.096831   1.357683                               \n",
      "    17     1.088615   1.358033                               \n",
      "    18     1.099059   1.358485                               \n",
      "    19     1.092452   1.358625                               \n",
      "    20     1.094024   1.359351                               \n",
      "    21     1.095973   1.359379                               \n",
      "    22     1.087525   1.359582                               \n",
      "    23     1.092964   1.359751                               \n",
      "    24     1.083159   1.359927                               \n",
      "    25     1.089971   1.359914                               \n",
      "    26     1.086155   1.360172                               \n",
      "    27     1.090148   1.359835                               \n",
      "    28     1.088475   1.360285                               \n",
      "    29     1.087271   1.360346                               \n",
      "    30     1.087093   1.360354                               \n",
      "    31     1.083889   1.360353                               \n",
      "    32     1.087923   1.36036                                \n",
      "    33     1.08515    1.360457                               \n",
      "    34     1.088837   1.360815                               \n",
      "    35     1.080709   1.361243                               \n",
      "    36     1.082725   1.361448                               \n",
      "    37     1.077955   1.362214                               \n",
      "    38     1.080972   1.362773                               \n",
      "    39     1.078188   1.362983                               \n",
      "    40     1.080087   1.363162                               \n",
      "    41     1.07415    1.363154                               \n",
      "    42     1.07836    1.363873                               \n",
      "    43     1.079702   1.364286                               \n",
      "    44     1.085103   1.364365                               \n",
      "    45     1.076593   1.364819                               \n",
      "    46     1.073      1.364948                               \n",
      "    47     1.071945   1.365435                               \n",
      "    48     1.073665   1.365318                               \n",
      "    49     1.075623   1.36556                                \n",
      "    50     1.067941   1.365611                               \n",
      "    51     1.067274   1.365714                               \n",
      "    52     1.067759   1.365879                               \n",
      "    53     1.072086   1.365971                               \n",
      "    54     1.067021   1.365977                               \n",
      "    55     1.074575   1.36654                                \n",
      "    56     1.069626   1.366598                               \n",
      "    57     1.066414   1.36638                                \n",
      "    58     1.074659   1.366684                               \n",
      "    59     1.066592   1.366519                               \n",
      "    60     1.070329   1.366687                               \n",
      "    61     1.065208   1.366682                               \n",
      "    62     1.066849   1.36672                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.36672])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end=lambda sched, cycle: save_model(m, f'language/models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those chiefly in commences of which may preferred asian sentiment, spiritually, cannot except themselves upon what he has been well to us, is a defocrate event these great are sacrifice for the true, when must athements, and think itwere that heredoubt, as has al.!153. he who enjoys howmuch more welfkind himself he security of knowledge is once doon something unconnecsion to overcome to ournevers ofth\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
